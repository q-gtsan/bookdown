[["index.html", "Self-learning material book Chapter 1 Vorwort 1.1 CoP 1.2 Einleitung zur Selbstlerneinheit 1.3 Typographische Konvention 1.4 Quelle/weiterführende Information 1.5 Feedback", " Self-learning material book Prof. Berenike Herrmann, Prof. Oliver Böhm-Kasper, Quy-Sing Glindemann-Tsan 2022-10-20 Chapter 1 Vorwort Dieses ‘bookdown’ dient zur Dokumentation eines interdisziplinären Seminars im Rahmen des Projektes BiLinked mit dem Arbeitsschwerpunkt (CoP) Data Literacy. Ergänzend werden die Inhalte des Seminars im Sinne der Nachhaltigkeit respektive Wiederverwendung als Selbstlerneinheit für Data Literacy zur Verfügung gestellt. 1.1 CoP Diese Community of Practice (CoP) zielt auf die Vermittlung und den Erwerb von „Data Literacy“ ab. “Der Begriff beschreibt die Fähigkeit, Daten zu sammeln, zu verwalten, auszuwerten, zu interpretieren und anzuwenden; immer verbunden mit einer kritischen Reflexion der Voraussetzungen und Ziele” (Schüller, Busch &amp; Hindinger, 2019, S. 10; Ridsdale et al., 2015). In der CoP soll die interdisziplinäre und kollaborative Zusammenarbeit von Studierenden und Lehrenden in projektförmig konzipierten Lehrveranstaltungen etabliert werden. Das Thema Data Literacy bietet zahlreiche Anknüpfungspunkte für realitätsnahe Fragestellungen und projektorientierte Arbeitsformen. Es ist damit ein guter Einstieg in die Entwicklung innovativer Lehrformate, die sich fächerübergreifend einsetzen lassen. 1.2 Einleitung zur Selbstlerneinheit Diese Selbstlerneinheit basiert auf der Grundlage des Seminars der Universität Bieleld: Erwachsenwerden in der Netz-Literatur. Mixed-methods Analysen von Social Media (S) (SoSe 2022). Für die Inhalte und Durchführung sind die Professorin Berenike Herrmann (Fakultät Literaturwissenschaft) und der Professor Oliver Böhm-Kasper (Fakultät Erziehungswissenschaft) verantwortlich gewesen. Auch unabhängig von der Teilnahme am Seminar können Lernende selbständig dieses Tutorial nutzen, um Twitter-Daten selbständig zu sammeln und auszuwerten. 1.3 Typographische Konvention Im Sinne der Herangehensweise das vorliegende Material als Selbstlerneinheit zu nutzen, existieren neben dem Haupttext unterschiedliche Textblöcke. Die Textblöcke werden im folgenden vorgestellt: In diesem Textblock stehen Übungen. In diesem Textblock werden auf notwendige Inhalte oder Ausführungen verwiesen. In diesem Textblock wird das Gerlernte überprüft. Wenn Unsicherheiten bei den Fragen bestehen, führen die dazugehörigen Links zu den Inhalten zum Nachlesen. 1.3.1 Feedback Am Ende eines Kapitels befindet sich ein Eingabefeld für (anonyme) Fragen, Anmerkungen/Anregungen und Rückemldungen. 1.4 Quelle/weiterführende Information Ridsdale, C. et al. (2015). Strategies and Best Practices for Data Literacy Education. Knowledge Synthesis Report. Zugriff am 05.05.2021 unter https://dalspace.library.dal.ca/handle/10222/64578 Schüller, K., Busch, P. &amp; Hindinger, C. (2019). Future Skills: Ein Framework für Data Literacy. Arbeitspapier Nr. 47. Berlin: Hochschulform Digitalisierung. Zugriff am 05.05.2021 unter https://hochschulforumdigitalisierung.de/sites/default/files/dateien/HFD_AP_Nr_47_DALI_Kompetenzrahmen_WEB.pdf. 1.5 Feedback "],["intro-twitter.html", "Chapter 2 Einführung in Social Media - Twitter 2.1 Was ist Twitter? 2.2 Umfrage: Welche Social Media nutzt du? 2.3 Ergebnis der Umfrage im Seminar: 2.4 Quelle/weiterführende Information 2.5 Feedback", " Chapter 2 Einführung in Social Media - Twitter 2.1 Was ist Twitter? “Twitter ist in erster Linie ein Echtzeitdienst zum Teilen von auf 280 Zeichen limitierten Text-Nachrichten (Tweets) in einem personalisierten, öffentlichen Nachrichtenstrom” (Jürgens &amp; Jungherr, 2011, S. 203). Zudem besticht Twitter durch die schnelle und ungefilterte Verbreitung von Informationen (Parmelee &amp; Bichard, 2012, S. 216). Der Dienst stellt ein weitverbreiteten Kommunikationskanal dar und wird oft als Kommunikationsplattform, soziales Netzwerk oder meist öffentlich einsehbares Online-Tagebuch definiert (Pfaffenberger, 2015, S. 26; Wikipedia.de, 2022). Mehr Informationen sind unter der angegebenen Literatur zu finden. 2.2 Umfrage: Welche Social Media nutzt du? Nimm’ gerne an der Umfrage teil, welche Social Media du nutzt 2.3 Ergebnis der Umfrage im Seminar: Unter den Top 3 waren die am häufigsten genutzten Social Media: WhatsApp Instagram YouTube 2.4 Quelle/weiterführende Information Jürgens, P., &amp; Jungherr, A. (2011). Wahlkampf vom Sofa aus: Twitter im Bundestagswahlkampf 2009. In E. J. Schweitzer &amp; S. Albrecht (Hrsg.), Das Internet im Wahlkampf. Analysen zur Bundestags- wahl 2009 (S. 201–225). Wiesbaden: VS Verlag für Sozialwissenschaften. Parmelee, J. H., &amp; Bichard, S. L. (2012). Politics and the Twitter revolution. How tweets influence the relationship between political leaders and the public (Lexington studies in political communica- tion). Lanham, Md: Lexington Books. Pfaffenberger, Fabian. Twitter als Basis wissenschaftlicher Studien: Eine Bewertung gängiger Erhebungs- und Analysemethoden der Twitter-Forschung. 1. Aufl. 2016. Wiesbaden: Springer Fachmedien Wiesbaden GmbH, 2016. Wikipedia: Twitter 2.5 Feedback "],["einführung-in-rstudio-cloud.html", "Chapter 3 Einführung in RStudio (Cloud) 3.1 Zugang 3.2 Erste Schritte 3.3 Knowledge-Check 3.4 Feedback", " Chapter 3 Einführung in RStudio (Cloud) Für die Durchführung von Twitter-Analysen wird ein Analyseprogramm benötigt, das die Daten sammmeln und auswerten kann. Hierzu wurde im Seminar mit RStudio gearbeitet - konkret mit RStudio Cloud. RStudio ist eine integrierte Entwicklungsumgebung und grafische Benutzeroberfläche für die Statistik-Programmiersprache R. 3.1 Zugang Unter folgendem Link kann das Programm RStudio für das jeweilige Betriebssystem heruntergeladen werden. Es besteht darüber hinaus die Möglichkeit einen RStudio Cloud Zugang bzw. Konto anzulegen, um über den Webbrowser eine webbasierte Version von RStudio zu benutzen. Die Verwendung der webbasierten Version hat den Voreil, dass die Programme R und RStudio nicht lokal installiert werden müssen und immer die aktuellen Versionen zur Verfügung stehen. In Seminaren werden zudem Probleme mit unterschiedlichen Betriebssystemen bei der Ausführung von RStudio vermieden. Die Anmeldung und Nutzung der RStudio Cloud ist bei der Verwendng eines Cloud Free-Planes (ausreichend für Lehr-/Lernkontexte) kostenlos. Für die Twitter-Analysen wird dies empfohlen. 3.1.1 Aufgabe Lade das Programm RStudio herunter + Installiere das Programm auf dem PC/Laptop Empfohlen: Leg ein Konto mit deiner Uni-Emailadresse an, um den Zugang zu RStudio Cloud zu erhalten. 3.1.2 Aufgabe Erstelle einen Twitter-Account Melde dich über die Developer Platform von Twitter mit Ihrem Acount an 3.2 Erste Schritte Im Internet existiert eine Vielzahl an guten Erläuterungen, die in RStudio einführen. An dieser Stelle wird auf das erste und zweite Kapitel des bookdowns von Ellis &amp; Mayer (2022) verwiesen. Um einen guten Überblick zu erhalten sollen folgende Kapitel durchgearbeitet werden: Kapitel 1 - RStudio Workflow Kapitel 2 - Die R Sprache 3.3 Knowledge-Check Kapitel 1 Wofür sind packages nützlich? An welcher Stelle können unbekannte Funktionen gesucht werden? Welche Zeichen werden häufig in R verwendet? Kapitel 2 Wie werden Variablen defniert? Welche Datentypen existieren in R? Welche Funktion verrät uns den Datentyp? Was sind Listen? Was ist der Unterschied zwischen einer Liste und einem DataFrame? 3.3.1 Ergänzende Video-Tutorials zum RStudio-Handling Ergänzend ist es sinnvoll praktische Instruktionen durch Video-Tutorials zu nutzen, damit die ersten Schritte im Umgang mit RStudio gelingen. Hierbei werden diese Tutorials empfohlen: gnis - Einführung in R/Rstudio Statistik am PC - R mit RStudio - eine Einführung in die Bedienung von RStudio Paul Borsdorf - RStudio-Tutorial: Skript nutzen 3.4 Feedback "],["einführung-in-die-twitter-api.html", "Chapter 4 Einführung in die Twitter API 4.1 Twitter API 4.2 Feedback", " Chapter 4 Einführung in die Twitter API 4.1 Twitter API Damit der Zugriff von RStudio Cloud/RStudio auf die Twitter API erfolgen kann, wird ein erhöhter Zugang seitens des eigenen Twitter Accounts benötigt. - Der Status lautet elevated access. Für die Beantragung solch eines Zugangs sind folgende Fragen zu beantworten: How will you use the Twitter API or Twitter Data? Are you planning to analyze Twitter Data? Do you plan to display Tweets or aggregate data about Twitter content outside Twitter? Schau dir zunächst das Video-Tutorial von AI Spectrum bis zur Minute 4:39 bzw. das Kapitel Creating Twitter Developer Account with Elevated Access an - YouTube. Erstelle nun einen Developer-Account wie im Tutorial: Erstelle eine APP Hinterlege die individuellen Infos zu deinem Account: API, API Key Secret, Bearer-Token Generiere und hinterlege deinen Access Token und Access Token Secret Bewerbe dich für den elevated access durch die Beantwortung der obigen Fragen Link zum Developer-Portal von Twitter Der konktrete Zugriff auf die Twitter API über RStudio wird im späteren Verlauf erläutert. An dieser Stelle wird ausdrücklich darauf hingewiesen, dass der Status: elevated acess erforderlich ist, um mit RStudio Cloud/ RStudio einen Zugriff auf die Twitter API zu erhalten. Zur Erstellung solch eines Zugangs ist die Angabe einer gültigen mobilen Rufnummer notwendig. Zur Erstellung solch eines Zugangs ist die Angabe einer gültigen mobilen Rufnummer in den Twitter Einstellungen notwendig. 4.2 Feedback "],["organisation-des-virtuellen-workplace.html", "Chapter 5 Organisation des virtuellen Workplace 5.1 Vorschlag für die Arbeitsorganisation 5.2 Feedback", " Chapter 5 Organisation des virtuellen Workplace Der erste Schritt ist die Vorbereitung des Arbeitsplatzes. Das betrifft Das Aufrufen von RStudio/RStudio Cloud Die Herstellung einer Arbeitsorganisation (Ordnerstruktur) Diese Schrittfolgen sind für alle Aufgaben gleich und werden vor der Bearbeitung einer AUfgabe voran gestellt. 5.1 Vorschlag für die Arbeitsorganisation 5.1.1 RStudio Bearbeitung einer neuen Aufgabe Öffne RStudio Erstelle ein Ordner mit einem sinnvollen Namen, damit du in der Zukunft auch noch weißt, wo deine Skripte zu finden sind Erstelle ein Projekt über den obigen Reiter “File -&gt; New Project”. Gib dem Projekt einen sinnvollen Namen. Anschließend erstelle in RStudio ein neues Skript. Speicher das Skript ebenfalls unter einem sinnvollen Namen in dem eben erstellten Ordner ab. Über das Skript werden die Aufgaben bearbeitet. Es ist meistens sinnvoll ein weiteres Skript zu erstellen, um bspw. eine neue oder eine Teilaufgabe zu bearbeiten. Das sorgt für mehr Übersicht. Bearbeitung einer bestehenden Aufgabe Gehe in den Ordner, in dem das Projekt abgespeichert ist. Öffne die Projektdatei mit der Endung (.rProj). Über diese Vorgehensweise, öffnet sich RStudio mit all den Skripten, die zuletzt im Projekt geöffnet wurden. 5.1.2 RStudio Cloud Bearbeitung einer neuen Aufgabe Log dich in RStudio Cloud ein Gehe in dein Workspace und erstelle ein neues Projekt mit einem sinnvollen Namen. Damit weißt du in der Zukunft auch noch, wo deine Skripte zu finden sind Anschließend erstelle in RStudio ein neues Skript. Speicher das Skript ebenfalls unter einem sinnvollen Namen in dem eben erstellten Ordner ab. Über das Skript werden die Aufgaben bearbeitet. Es ist meistens sinnvoll ein weiteres Skript zu erstellen, um bspw. eine neue oder eine Teilaufgabe zu bearbeiten. Das sorgt für mehr Übersicht. Bearbeitung einer bestehenden Aufgabe Log dich in RStudio Cloud ein und gehe in dein Workspace, in dem das Projekt abgespeichert ist. Öffne das Projekt Über diese Vorgehensweise, öffnet sich RStudio mit all den Skripten, die zuletzt im Projekt geöffnet wurden. 5.2 Feedback "],["erste-lerneinheit---twitter-authentifizierung-erste-analysen.html", "Chapter 6 Erste Lerneinheit - Twitter Authentifizierung &amp; erste Analysen” 6.1 Workspace erstellen 6.2 Authentifizierung 6.3 Fingerübung 6.4 Quelle 6.5 Feedback", " Chapter 6 Erste Lerneinheit - Twitter Authentifizierung &amp; erste Analysen” Für die anschließende Bearbeitung sind der Zugang zu RStudio Cloud/RStudio sowie der Status elevated Access im Twitter-Developer Account erforderlich. Gehe in die entsprechenden Kapitel zurück, wenn die Voraussetzungen noch nichte erfüllt sind. 6.1 Workspace erstellen Bereite dein Workplace vor (siehe Kapitel 4). Öffne ein leeres Skript. 6.2 Authentifizierung 6.2.1 Einlesen von notwendigen Paketen Zunächst werden im Skript die erforderlichen Pakete installiert und aufgerufen. Die Pakete beinhalten bestimmte Funktionen, die nicht im Basis-Umfang enthalten sind. Diese Funktionen sind für die Twitter-API und die ersten Analysen wichtig. Code # load twitter library library(rtweet) # plotting and pipes - tidyverse! library(tidyverse) # text mining library library(tidytext) Code # install packages install.packages(c(&quot;rtweet&quot;,&quot;tidytext&quot;,&quot;tidyverse&quot;)) # load twitter library library(rtweet) # plotting and pipes - tidyverse! library(tidyverse) # text mining library library(tidytext) 6.2.2 Erstellen eines Authentifikations-Token Im nächsten Schritt wird eine Verbindung zur Twitter API über das Paket rtweet hergestellt. Dafür müssen die Informationen (Bearer-Token), die im 4.1 hinterlegt wurden, eingegeben werden. Code # rtweet_app() Funktion um die eigene Twitter APP zu authentifizieren. # Das ist die naheliegendste Funktionin Bezug auf das Sammeln von Daten auth &lt;- rtweet_app( bearer_token = &quot;dein_bearer_token&quot; ) # authenticate your rtweet app through the function &#39;auth_as()&#39; with the saved ojbect &#39;auth&#39; auth_as(auth) Sofern alles geklappt hat, erscheint keine Fehlermeldung. Im nächsten Code-Abschnitt testen wir die Verbindung zur Twitter-API mit einer Twitter-Suche nach dem Hashtag #Bielefeld. Die Ergebnisse werden in dem Objekt bielefeld_tweets gespeichert. 6.2.3 Einlesen von Twitter-Daten Code # Erster Abruf von Twitter-Daten bielefeld_tweets &lt;- search_tweets(q = &quot;#Bielefeld&quot;, n = 500) 6.2.4 Ergebnisse Nachdem die Tweets heruntergeladen wurden, können wir nun einen Blick auf die Ergebnisse werfen: Code #Übersicht über Datensatz #Übersicht über die Struktur des Datensatzes head (bielefeld_tweets) ## # A tibble: 6 × 43 ## created_at id id_str full_…¹ trunc…² displ…³ entities ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 2022-10-19 14:10:58 1.58e18 158270586634… &quot;Seit … FALSE 279 &lt;named list&gt; ## 2 2022-10-20 15:18:00 1.58e18 158308512372… &quot;#Guet… FALSE 277 &lt;named list&gt; ## 3 2022-10-20 15:17:53 1.58e18 158308509556… &quot;#RB61… FALSE 179 &lt;named list&gt; ## 4 2022-10-20 15:09:22 1.58e18 158308295105… &quot;Hallo… FALSE 240 &lt;named list&gt; ## 5 2022-10-20 15:02:16 1.58e18 158308116368… &quot;#RE82… FALSE 155 &lt;named list&gt; ## 6 2022-10-20 15:00:01 1.58e18 158308059886… &quot;#RB61… FALSE 129 &lt;named list&gt; ## # … with 36 more variables: metadata &lt;list&gt;, source &lt;chr&gt;, ## # in_reply_to_status_id &lt;dbl&gt;, in_reply_to_status_id_str &lt;chr&gt;, ## # in_reply_to_user_id &lt;dbl&gt;, in_reply_to_user_id_str &lt;chr&gt;, ## # in_reply_to_screen_name &lt;chr&gt;, geo &lt;list&gt;, coordinates &lt;list&gt;, ## # place &lt;list&gt;, contributors &lt;lgl&gt;, is_quote_status &lt;lgl&gt;, ## # retweet_count &lt;int&gt;, favorite_count &lt;int&gt;, favorited &lt;lgl&gt;, ## # retweeted &lt;lgl&gt;, possibly_sensitive &lt;lgl&gt;, lang &lt;chr&gt;, … Code #Übersicht über die Variablennamen (Spaltennamen) names (bielefeld_tweets) ## [1] &quot;created_at&quot; &quot;id&quot; ## [3] &quot;id_str&quot; &quot;full_text&quot; ## [5] &quot;truncated&quot; &quot;display_text_range&quot; ## [7] &quot;entities&quot; &quot;metadata&quot; ## [9] &quot;source&quot; &quot;in_reply_to_status_id&quot; ## [11] &quot;in_reply_to_status_id_str&quot; &quot;in_reply_to_user_id&quot; ## [13] &quot;in_reply_to_user_id_str&quot; &quot;in_reply_to_screen_name&quot; ## [15] &quot;geo&quot; &quot;coordinates&quot; ## [17] &quot;place&quot; &quot;contributors&quot; ## [19] &quot;is_quote_status&quot; &quot;retweet_count&quot; ## [21] &quot;favorite_count&quot; &quot;favorited&quot; ## [23] &quot;retweeted&quot; &quot;possibly_sensitive&quot; ## [25] &quot;lang&quot; &quot;retweeted_status&quot; ## [27] &quot;quoted_status_id&quot; &quot;quoted_status_id_str&quot; ## [29] &quot;quoted_status&quot; &quot;text&quot; ## [31] &quot;favorited_by&quot; &quot;scopes&quot; ## [33] &quot;display_text_width&quot; &quot;quoted_status_permalink&quot; ## [35] &quot;quote_count&quot; &quot;timestamp_ms&quot; ## [37] &quot;reply_count&quot; &quot;filter_level&quot; ## [39] &quot;query&quot; &quot;withheld_scope&quot; ## [41] &quot;withheld_copyright&quot; &quot;withheld_in_countries&quot; ## [43] &quot;possibly_sensitive_appealable&quot; Code #Inhalte der ersten 10 Tweets anzeigen bielefeld_tweets$text[1:10] ## [1] &quot;Seit fast 2 Jahren recherchieren wir zu einem unfassbaren Verbrechen in der #Bielefeld​er #Bethel-Klinik: Ein Assistenzarzt betäubte und vergewaltigte rund 30 Patientinnen. Unsere Reportage dazu heute 22:15 im rbb Fernsehen, jetzt schon in der Mediathek:\\n\\nhttps://t.co/gH1QibgmiX https://t.co/PQRhhR6bKx&quot; ## [2] &quot;#Guetersloh #Bielefeld #Osnabrueck @RadioGuetersloh @RadioBielefeld @nwnews @westfalenblatt #HalleWestfalen #Wub \\nIch freue mich drauf, mit euch zusammen zu essen und zu lachen! Lesung am 29.10. im Café Altes Pfarrhaus in Halle Westf. Reservierung hier: https://t.co/JCyT2zh5xf https://t.co/a1slLaBtgr&quot; ## [3] &quot;#RB61 #Hengelo - #Bielefeld Hbf | Verspätungen | von Wissingen bis Bielefeld Hbf | Technische Störung an der Strecke | Störung von 15:15 bis vsl. 16:00 Uhr https://t.co/Z8Er2H7m3I&quot; ## [4] &quot;Hallo Ostwestfalen! \\nAm Sonntag, den 30.11., bin ich ins Bauernhausmuseum nach #Bielefeld eingeladen. Ich freue mich sehr auf Vortrag &amp;amp; Diskussion über #Biodiversität in meiner alten Heimat OWL.\\nKommt zahlreich!\\n\\nhttps://t.co/nEFpHUdHC0&quot; ## [5] &quot;#RE82 #Bielefeld Hbf - #Altenbeken | Verspätungen | von Altenbeken bis Altenbeken | Zugfolge | Störung von 15:01 bis vsl. 15:09 Uhr https://t.co/Z8Er2H7m3I&quot; ## [6] &quot;#RB61 #Hengelo 15:34 nach #Bielefeld Hbf 17:48 #Teilausfall von #Hengelo bis Oldenzaal aufgrund eines nicht NL-fähigen Fahrzeugs.&quot; ## [7] &quot;RT @eurobahn_info: #RE78 #Nienburg (Weser) - #Bielefeld Hbf (beide Ri.) | Zugausfälle | von Nienburg (Weser) bis Bielefeld Hbf (beide Ri.)…&quot; ## [8] &quot;#RE78 #Nienburg (Weser) - #Bielefeld Hbf (beide Ri.) | Zugausfälle | von Nienburg (Weser) bis Bielefeld Hbf (beide Ri.) | Kurzfristiger Personalausfall | Störung von 19:24 bis vsl. 01:05 Uhr https://t.co/Z8Er2Hop5I&quot; ## [9] &quot;Revenge😈 #VfB #vfbstuttgart #bielefeld https://t.co/jt3OQKaK9M&quot; ## [10] &quot;RT @ZuginfoNRW: #RE78 #Zugausfall #Bielefeld Hbf 15:24 - #Nienburg (Weser) 16:49. Aufgrund eines kurzfristigen Personalausfalls.&quot; So sieht der Output aus, wenn die Authentifikation und die Suche geklappt haben. 6.2.5 Abspeichern Zur Erleichterung speichern wir unsere Authentifizierung ab, um die erneute Authentifizierung beim nächsten Mal nicht wiederholen zu müssen. Hierbei nutzen wir die Funktion auth_save aus dem rtweet-Paket, um die Zugangsdaten unter den Namen “Meine App” abzuspeichern. Code # Abspeichern der Zugangsdaten auth_save(auth, &quot;Meine App&quot;) ## Saving auth to &#39;/Users/q.t./Library/Preferences/org.R-project.R/R/rtweet/Meine ## App.rds&#39; Wenn wir in der nächsten Session die Twitter API benutzen möchten, rufen wir folgenden Befehl mit dem selbstbestimmten Namen “Meine App” für die Authentifizierung auf. Code auth_as(&quot;Meine App&quot;) 6.3 Fingerübung Arbeite folgende Übungen mithilfe der rtweet Dokumentation durch: Link. Hierbei ist das Ziel sich mit den Werkzeugen vertraut zu mahen. Suche nach dem Hashtag “micropeotry” und speicher die Ergebnisse ab. Schau dir die tweets an. Suche nach den user IDs der Twitter-Accounts, die die Universität Bielefeld “@unibielefeld” folgt Suche nach Followern von der Uni Bielefeld “@unibielefeld” Suche nach der Timeline von Bielefeld 6.4 Quelle Blogbeitrag zur Nutzung des rtweet-Pakets 6.5 Feedback "],["beispiel-analyse.html", "Chapter 7 Beispiel-Analyse 7.1 Workplace &amp; Authentifizierung Twitter 7.2 Tweets-Suche 7.3 Häufigste User der Hashtags #Bielefeld oder #bielefeld 7.4 Quelle 7.5 Feedback", " Chapter 7 Beispiel-Analyse In diesem Kapitel werden die Tweets mit dem Hashtag #Bielefeld respektive #bielefeld untersucht und visualisiert. Setzt dich mit dem Kapitel auseinander Such dir einen Hashtag aus und führe die Methoden und Funktionen für diesen Hashtag wie in diesem Kapitel aus Der Sinn dieser Übung ist es, die Programmiersprache R respektive RStudio kennenzulernen. 7.1 Workplace &amp; Authentifizierung Twitter An dieser Stelle wird nur auf die Herstellung des virtuellen Workplaces hingewiesen. Des Weiteren benutzen wir nun mithilfe der Funktion ’auth_as(“Meine APP”)’die vereinfachte Authentifikations-Methode; ohne Zugangsinformationen eingeben zu müssen. Zunächst wird das notwendige Paket rtweet eingelesen. Code # Einlesen des notwendigen Packages library(rtweet) auth_as(&quot;Meine App&quot;) 7.2 Tweets-Suche Wir speichern die Tweets in dem Objekt “Bielefeld_tweets” ab. Die Analyse und die Visualierung basiert auf dem Objekt, der die notwendigen Daten enthält. Code # Abruf von Tweets unter den Hashtags #Bielefeld oder #bielefeld Bielefeld_tweets &lt;- search_tweets(&quot;#Bielefeld OR #bielefeld&quot;, n=9000, #Suche nach Tweets #Bielefeld oder #bielefeld, max. 9000 Tweets lang = &quot;de&quot;, #Sprache der Tweets: Deutsch include_rts = TRUE) #Retweets sollen inkludiert sein 7.3 Häufigste User der Hashtags #Bielefeld oder #bielefeld Um die zehn häufigsten User aus dem Datensatz zu erhalten, wird zur Vereinfachung des Programmierens das Paket tidyverse benutzt. Das Paket bietet die Möglichkeit direkt über den Code einen leserfreundlicheren (&amp; besser nachvollziehbaren) Code zu erstellen, ohne viel kommentieren zu müssen. Für die Auflistung der häufigsten User sind die Account-Namen notwendig, die über die Funktion users_data(Bielefeld_tweets) eingeholt werden. Dies wird in dem Objekt tweets_bi abgespeichert. Code #Notwendiges Paket für die pipe &quot; %&gt;% &quot;-Programmierung library (tidyverse) tweets_bi &lt;- users_data(Bielefeld_tweets) #u.a. die Daten zum Accout-Namen einholen tweets_bi %&gt;% #Zugriff auf Daten mit #Bielefeld-Tweets count (name) %&gt;% #Anzahl der Twitter-Nutzer-Namen top_n(10) %&gt;% #Top 10 werden ausgewählt arrange(desc(n)) #in absteigender Reihenfolge ausgeben ## # A tibble: 17 × 2 ## name n ## &lt;chr&gt; &lt;int&gt; ## 1 eurobahn 296 ## 2 (Kurzfristiger) Personalausfall 129 ## 3 zuginfo.nrw 99 ## 4 Tempo-Team Personaldienstleistungen GmbH 23 ## 5 Polizei NRW BI 15 ## 6 Neue Westfälische 12 ## 7 Fuchsbaubewohner 🦊 7 ## 8 iranprotestsgermany 6 ## 9 Michael Gugat 5 ## 10 ███.dissent.is/████████████ 4 ## 11 hallo24.de 4 ## 12 Kontraste 4 ## 13 LOUISEXYZJEANS VON DE RÖYAL 4 ## 14 presseportal.de 4 ## 15 Rosella Wenninger 4 ## 16 Simon N. ~ 💙🇩🇪 4 ## 17 Westfalen-Blatt 4 7.3.1 Visualisierung der Daten: 7.3.1.1 Über eine Tabelle mit dem Paket gt Um eine ansprechendere Visualisierung zu generieren, nutzen wir das Paket gt. Dafür ist das Paket zu installieren: Code install.packages(&quot;gt&quot;) Anschließend wird das Paket eingelesen und die Tabelle wird erstellt. Die Dokumentation zu dem Paket sind optional hier nachzulesen. Code #Notwendiges Paket für gt-Tables library (gt) tweets_bi %&gt;% count (name) %&gt;% top_n(10) %&gt;% arrange(desc(n)) %&gt;% gt() %&gt;% #Erzeugung der Tabelle tab_header ( #&quot;Kopf&quot; der Tabelle layouten title = &quot;Häufigste User des Hashtags #Bielefeld oder #bielefeld&quot;, #Tabellen-Titel subtitle = &quot;Absolute Anzahl&quot;) %&gt;% # Untertitel cols_label(name = &quot;Twitter-Nutzer-Name&quot;, n = &quot;Anzahl der Tweets&quot;) %&gt;% #Beschriftung der Spalten tab_source_note( source_note = paste(&quot;Source: Data collected from TwitterAPI via rtweet-package &quot;,Sys.Date()) ) #Angabe der Quellen html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #bmzqheolfd .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #bmzqheolfd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bmzqheolfd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #bmzqheolfd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #bmzqheolfd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bmzqheolfd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bmzqheolfd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #bmzqheolfd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #bmzqheolfd .gt_column_spanner_outer:first-child { padding-left: 0; } #bmzqheolfd .gt_column_spanner_outer:last-child { padding-right: 0; } #bmzqheolfd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #bmzqheolfd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #bmzqheolfd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #bmzqheolfd .gt_from_md > :first-child { margin-top: 0; } #bmzqheolfd .gt_from_md > :last-child { margin-bottom: 0; } #bmzqheolfd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #bmzqheolfd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #bmzqheolfd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #bmzqheolfd .gt_row_group_first td { border-top-width: 2px; } #bmzqheolfd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bmzqheolfd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #bmzqheolfd .gt_first_summary_row.thick { border-top-width: 2px; } #bmzqheolfd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bmzqheolfd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bmzqheolfd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #bmzqheolfd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #bmzqheolfd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bmzqheolfd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bmzqheolfd .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #bmzqheolfd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bmzqheolfd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #bmzqheolfd .gt_left { text-align: left; } #bmzqheolfd .gt_center { text-align: center; } #bmzqheolfd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #bmzqheolfd .gt_font_normal { font-weight: normal; } #bmzqheolfd .gt_font_bold { font-weight: bold; } #bmzqheolfd .gt_font_italic { font-style: italic; } #bmzqheolfd .gt_super { font-size: 65%; } #bmzqheolfd .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #bmzqheolfd .gt_asterisk { font-size: 100%; vertical-align: 0; } #bmzqheolfd .gt_indent_1 { text-indent: 5px; } #bmzqheolfd .gt_indent_2 { text-indent: 10px; } #bmzqheolfd .gt_indent_3 { text-indent: 15px; } #bmzqheolfd .gt_indent_4 { text-indent: 20px; } #bmzqheolfd .gt_indent_5 { text-indent: 25px; } Häufigste User des Hashtags #Bielefeld oder #bielefeld Absolute Anzahl Twitter-Nutzer-Name Anzahl der Tweets eurobahn 296 (Kurzfristiger) Personalausfall 129 zuginfo.nrw 99 Tempo-Team Personaldienstleistungen GmbH 23 Polizei NRW BI 15 Neue Westfälische 12 Fuchsbaubewohner 🦊 7 iranprotestsgermany 6 Michael Gugat 5 ███.dissent.is/████████████ 4 hallo24.de 4 Kontraste 4 LOUISEXYZJEANS VON DE RÖYAL 4 presseportal.de 4 Rosella Wenninger 4 Simon N. ~ 💙🇩🇪 4 Westfalen-Blatt 4 Source: Data collected from TwitterAPI via rtweet-package 2022-10-20 7.3.1.2 Über ein Diagram mit dem Paket ggplot2 Für diese Visualisierung wird das Paket ggplot2 benötigt, welches zuvorderst eingelesen wird (und wenn notwendig installiert wird). Code install.packages(&quot;ggplot2&quot;) Die Erstellung und Ausgabe des Diagramms erfolgt in diesem Abschnitt. Code #notwendiges Paket library(ggplot2) #Als Graphik tweets_bi %&gt;% count(name, sort = TRUE) %&gt;% na.omit() %&gt;% top_n(10) %&gt;% ggplot(aes(reorder (x =name, n), y = n)) + #Erzeugung eines Diagramms geom_col(fill = &#39;steelblue4&#39;) + #Farbe der Balken coord_flip() + #Diagramm um 90 Grad gedreht labs(x = &quot;Name&quot;, #Beschriftung der X-Achse y = &quot;Anzahl&quot;, #Beschriftung der y-Achse title = &quot;Who is tweeting? &quot;, #Diagrammtitel subtitle = &quot;under Hashtag #Bielefeld or #Bielefeld&quot;, #Untertitel des Diagramms caption = paste(&quot;Source: Data collected from TwitterAPI via rtweet-package &quot;,Sys.Date())) #Quellenangabe für Diagramm In der obigen Darstellung werden die Daten in einem Balkendiagramm präsentiert. Es ist auch möglich sich die Anzahl der Tweets pro Tag anzeigen zu lassen. Hier wird das Paket lubridate benötigt, welches installiert und eingelesen wird. Code #notwendiges Paket install.packages(&quot;lubridate&quot;) Im Anschluss werden die notwendigen Daten definiert und in einer Zeitreihe durch Balken dargestellt. Code library (lubridate) Bielefeld_tweets$date = date(Bielefeld_tweets$created_at) #Abspeicherung einer neuen Variable Datum Bielefeld_tweets_date = data.frame(table(Bielefeld_tweets$date)) #Erzeugung eines neuen Datensatz mit Datum und Anzahl der Tweets colnames(Bielefeld_tweets_date) = c(&#39;Datum&#39;,&#39;Total.Tweets&#39;) #Umbenennung der Variablennamen # Create data viz ggplot(Bielefeld_tweets_date)+ #unterste Ebene des Diagramm erzeugen geom_bar(aes(x = Datum, #Spezifizierung X-Achse y = Total.Tweets, #Spezifizierung y-Achse fill = I(&#39;cornflowerblue&#39;)), #Farbe der Balken stat = &#39;identity&#39;, #Verwendung der absoluten Werte aus dem Data.Frame show.legend = FALSE)+ #keine Legende anzeigen geom_hline(yintercept = mean(Bielefeld_tweets_date$Total.Tweets), #horizontale Linie basierend auf dem Mittelwert der Anzahl an Tweets col = (&#39;red&#39;), #Farbe rot size = 0.5)+ #Dicke der Linie geom_text(aes(fontface = &#39;italic&#39;, #kursiver Text label = paste(&#39;Mittelwert:&#39;, ceiling(mean(Bielefeld_tweets_date$Total.Tweets)), #Einfügen des automatisch berechneten Mittelwertes &#39;Tweets pro Tag&#39;), x = 1, #Position des Texts auf der X-Achse (erstes Datum) y = mean(Bielefeld_tweets_date$Total.Tweets)+20), #Position auf der Y-Achse, +20 über dem ersten Balken hjust = &#39;left&#39;, #Textposition links size = 4, #Textgröße col = (&#39;red&#39;) )+ #Textfarbe labs(title = &#39;Anzahl der Tweets pro Tag &#39;, #Diagrammtitel subtitle = &#39;mit dem Hashtag #Bielefeld oder #bielefeld&#39;, #Untertitel des Diagramms caption = paste(&quot;Source: Data collected from TwitterAPI via rtweet-package &quot;,Sys.Date()))+ xlab(&#39;Datum&#39;)+ #Beschriftung x-Achse ylab(&#39;Anzahl der Tweets&#39;)+ #Beschriftung y-Achse theme_bw() #Diagramm-Theme schwarz-weiß 7.4 Quelle Zur Homepage des Pakets: ggplot2 Zur Homepage des Pakets: gt Zur Homepage des Pakets: lubridate Zur Homepage des Pakets: tidyverse 7.5 Feedback "],["freq_table_analysis.html", "Chapter 8 Zweite Lerneinheit - Twitter Analyse: Häufigkeitstabelle &amp; Wordcloud 8.1 Vorbereitung 8.2 Twitter Suche 8.3 Data Pre-Processing: Cleaning 8.4 Data Pre-Processing: Transformieren 8.5 Data Visualization: Häufigkeitstabelle 8.6 Eigene Stopwörter erstellen 8.7 Data Visualization: Word-Cloud 8.8 Quelle 8.9 Feedback", " Chapter 8 Zweite Lerneinheit - Twitter Analyse: Häufigkeitstabelle &amp; Wordcloud Dieser Abschnitt behandelt die Visualisierung einer Häufigkeitstabelle und einer Wordcloud auf der Grundlage von twitter-Daten. 8.1 Vorbereitung Dafür ist es hilfreich sich vorbereitend die Website „Work With Twitter Social Media Data in R - An Introduction - Lesson 3: “Text Mining Twitter Data With TidyText in R” anzuschauen. In diesem Eintrag wird die Bedeutung der Begriffe wie ‘Data Munging’ respektive ‘Data Pre-Processing’ für die Visualierung hervorgehoben und deren Handhabarkeit in R knapp erläutert. 8.2 Twitter Suche Für diese Analyse werden wir nach den Hashtags #Erziehung und #Lyrik suchen. Vorab ist die Authentifizierung mit der Twitter-API notwendig. Code #Abruf von Tweets unter den Hashtags #Erziehung &amp; #Lyrik erziehung_tweets &lt;- search_tweets(&quot;#Erziehung&quot;, n=10000, lang = &quot;de&quot;, include_rts = TRUE) lyrik_tweets &lt;- search_tweets(&quot;#Lyrik&quot;, n=10000, lang = &quot;de&quot;, include_rts = TRUE) 8.3 Data Pre-Processing: Cleaning Ein wesentlicher Bestandteil der Datensammlung respektive der Datenbeschaffung ist die Bereinigung der Daten. Hierbei werden beispielsweise auf Fehler und Inkonsistenz der Daten geprüft. Des Weiteren werden irrelevante und korrupte Daten identifiziert und korrigiert. Mit diesem Schritt wird dafür gesorgt, dass die Daten, die wir präsentieren und weiterverarbeiten wollen konsistent und valide sind. Vorerst schauen wir uns die Twitter Beiträge an. Code # Anzeige der ersten Einträge in der Variable &quot;text&quot; head (erziehung_tweets$text) ## [1] &quot;RT @princessfeetvie: Guten Morgen, Ich vermisse den Sommer🥺❤️ \\n\\nMeldet euch bei mir per DM wenn ihr Unterwürfig seit..\\n@princessfeetvie \\n\\n#…&quot; ## [2] &quot;RT @princessfeetvie: Hallo wer will einer jungen Herrin dienen?\\nSchreibt mich an. DM ❤️\\n#domina #feet #soles #findom #femdom #cuckold #cuck…&quot; ## [3] &quot;RT @MissSarina4: Kriech nähe#Nylonlove #Genuss #Nylon #Anbeten #Dienen #Mistress #Fetisch #Fuß #Kiarashoes #Erniedrigung #Pantolette #Onlin…&quot; ## [4] &quot;RT @irisdarkphanta1: #erziehung ist sehr vielfältig…. Hier eine unserer Variationen ….. @Tom_PHoToToM https://t.co/JdUUgY4iAG&quot; ## [5] &quot;Kriech nähe#Nylonlove #Genuss #Nylon #Anbeten #Dienen #Mistress #Fetisch #Fuß #Kiarashoes #Erniedrigung #Pantolette #Onlineerziehung #Fußfetisch #nylonfetish #Fußverehrung #highheels #Fuß #Feinstrumpfhose #Anbetung #Fetisch #Fuß #Fetischmahlzeit #Heels #session #Erziehung https://t.co/oKRQtiEYk5&quot; ## [6] &quot;Bertelsmann-Studie zu Kitas löst kontroverse Reaktionen aus #Bildung #Sachsen #Erziehung #Familie #Dresden https://t.co/Vod5AygzyC&quot; Code head (lyrik_tweets$text) ## [1] &quot;RT @o_franco_aleman: O komm,\\nKomm zu mir,\\nIch bin ja so süß nach dir.\\nIch deine Lebendige,\\nDeine weilende Zier,\\nVergehe nach dir.\\n\\nPeter Hi…&quot; ## [2] &quot;#FreieTexte #FreieTexte #Lyrik #Ichwollteswäreallesnurschön #Gedicht Ich wollt, es wäre alles nur schön!: Ich wollt`, es wäre alles nur schön – jeder würde den anderen allein als Glücksfall seh`n! Ein Miteinander wäre weit und breit und nirgendwo… https://t.co/bjrrWP5Xgn&quot; ## [3] &quot;RT @o_franco_aleman: O komm,\\nKomm zu mir,\\nIch bin ja so süß nach dir.\\nIch deine Lebendige,\\nDeine weilende Zier,\\nVergehe nach dir.\\n\\nPeter Hi…&quot; ## [4] &quot;RT @o_franco_aleman: O komm,\\nKomm zu mir,\\nIch bin ja so süß nach dir.\\nIch deine Lebendige,\\nDeine weilende Zier,\\nVergehe nach dir.\\n\\nPeter Hi…&quot; ## [5] &quot;RT @o_franco_aleman: Im Namen der #Liebe \\nverschenken wir das #Herz. \\nIch verblute.\\n\\nPeter Turrini\\n\\n#lyrik #poesie #fotografie https://t.co…&quot; ## [6] &quot;RT @o_franco_aleman: O komm,\\nKomm zu mir,\\nIch bin ja so süß nach dir.\\nIch deine Lebendige,\\nDeine weilende Zier,\\nVergehe nach dir.\\n\\nPeter Hi…&quot; Als erstes wird der HTML-Text entfernt. Dabei werden die bereinigten Texte in einer neuen Spalte “stripped_text” abgespeichert. Code # Bereinigung um URLs und teilweise html-function # Bereinigung um URLS erziehung_tweets$stripped_text &lt;- gsub(&quot;http.*&quot;,&quot;&quot;, erziehung_tweets$text) erziehung_tweets$stripped_text &lt;- gsub(&quot;https.*&quot;,&quot;&quot;, erziehung_tweets$stripped_text) # Bereinigung um URLS lyrik_tweets$stripped_text &lt;- gsub(&quot;http.*&quot;,&quot;&quot;, lyrik_tweets$text) lyrik_tweets$stripped_text &lt;- gsub(&quot;https.*&quot;,&quot;&quot;, lyrik_tweets$stripped_text) 8.3.1 Aufgabe Um die Schritte der Datenbereinigung praktisch nachzuvollziehen, bereinige die Texteinträge erneut durch und entferne die HTML-Syntax-Zeichen: &amp;amp; mit &amp; und die \\n mit \" \"(Leerzeichen) . Nach der ersten groben Bereinigung betrachten wir nun das Ergebnis. Code head (erziehung_tweets$stripped_text) ## [1] &quot;RT @princessfeetvie: Guten Morgen, Ich vermisse den Sommer🥺❤️ Meldet euch bei mir per DM wenn ihr Unterwürfig seit.. @princessfeetvie #…&quot; ## [2] &quot;RT @princessfeetvie: Hallo wer will einer jungen Herrin dienen? Schreibt mich an. DM ❤️ #domina #feet #soles #findom #femdom #cuckold #cuck…&quot; ## [3] &quot;RT @MissSarina4: Kriech nähe#Nylonlove #Genuss #Nylon #Anbeten #Dienen #Mistress #Fetisch #Fuß #Kiarashoes #Erniedrigung #Pantolette #Onlin…&quot; ## [4] &quot;RT @irisdarkphanta1: #erziehung ist sehr vielfältig…. Hier eine unserer Variationen ….. @Tom_PHoToToM &quot; ## [5] &quot;Kriech nähe#Nylonlove #Genuss #Nylon #Anbeten #Dienen #Mistress #Fetisch #Fuß #Kiarashoes #Erniedrigung #Pantolette #Onlineerziehung #Fußfetisch #nylonfetish #Fußverehrung #highheels #Fuß #Feinstrumpfhose #Anbetung #Fetisch #Fuß #Fetischmahlzeit #Heels #session #Erziehung &quot; ## [6] &quot;Bertelsmann-Studie zu Kitas löst kontroverse Reaktionen aus #Bildung #Sachsen #Erziehung #Familie #Dresden &quot; Code head (lyrik_tweets$stripped_text) ## [1] &quot;RT @o_franco_aleman: O komm, Komm zu mir, Ich bin ja so süß nach dir. Ich deine Lebendige, Deine weilende Zier, Vergehe nach dir. Peter Hi…&quot; ## [2] &quot;#FreieTexte #FreieTexte #Lyrik #Ichwollteswäreallesnurschön #Gedicht Ich wollt, es wäre alles nur schön!: Ich wollt`, es wäre alles nur schön – jeder würde den anderen allein als Glücksfall seh`n! Ein Miteinander wäre weit und breit und nirgendwo… &quot; ## [3] &quot;RT @o_franco_aleman: O komm, Komm zu mir, Ich bin ja so süß nach dir. Ich deine Lebendige, Deine weilende Zier, Vergehe nach dir. Peter Hi…&quot; ## [4] &quot;RT @o_franco_aleman: O komm, Komm zu mir, Ich bin ja so süß nach dir. Ich deine Lebendige, Deine weilende Zier, Vergehe nach dir. Peter Hi…&quot; ## [5] &quot;RT @o_franco_aleman: Im Namen der #Liebe verschenken wir das #Herz. Ich verblute. Peter Turrini #lyrik #poesie #fotografie &quot; ## [6] &quot;RT @o_franco_aleman: O komm, Komm zu mir, Ich bin ja so süß nach dir. Ich deine Lebendige, Deine weilende Zier, Vergehe nach dir. Peter Hi…&quot; 8.4 Data Pre-Processing: Transformieren Für die Darstellung der Daten in einer Häufigkeitstabelle respektive in einer Wordcloud, müssen wir die Texte mithilfe den Paketen dplyr und tidytext in einzelne Wörter zerlegen. Code ##Tweets in einzelne Worte zerlegen erziehung_tweets_clean &lt;- erziehung_tweets %&gt;% select(stripped_text) %&gt;% unnest_tokens(word, stripped_text) lyrik_tweets_clean &lt;- lyrik_tweets %&gt;% select(stripped_text) %&gt;% unnest_tokens(word, stripped_text) 8.5 Data Visualization: Häufigkeitstabelle Nun werfen wir einen ersten Blick auf den Stand der Daten über eine Häufigkeitstabelle. Mithilfe des Pakets ggplot2. visualisieren wir die Twitter Daten: erziehung_tweets_clean Code erziehung_tweets_clean %&gt;% count(word, sort = TRUE) %&gt;% top_n(15) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x = word, y = n)) + geom_col(fill=&#39;steelblue4&#39;) + xlab(NULL) + coord_flip() + labs(x = &quot;Anzahl&quot;, y = &quot;Wörter&quot;, title = &quot;Count of unique words No. 1&quot;, subtitle=&quot;Tweets using #Erziehung&quot;, caption = paste(&quot;Source: Data collected from TwitterAPI via rtweet-package &quot;,Sys.Date())) Offensichtlich enthält die Tabelle noch Wörter, die wenig aussagekräftig sind. Aus diesem Grund erscheint es sinnvoll die Daten um deutsche ‘Stopwords’ zu bereinigen. Hierfür verwenden wir das Paket stopwords. Falls das Paket noch nicht in der RStudio beziehungsweise in der RStudio Cloud Bibliothek vorhanden ist, installiere das Paket und lies es ein. Code #Bereinigung um deutsche Stop-Words stopwords_de &lt;- tibble(word = stopwords::stopwords(language = &quot;de&quot;,source=&quot;stopwords-iso&quot;)) erziehung_tweets_clean &lt;- erziehung_tweets_clean %&gt;% anti_join(stopwords_de, by=&quot;word&quot;) lyrik_tweets_clean &lt;- lyrik_tweets_clean %&gt;% anti_join(stopwords_de, by=&quot;word&quot;) Nach diesem Vorgang werfen wir erneut einen Blick auf die Häufigkeitstabelle. Code ##Erneute Ausgabe der häufigsten Wörter erziehung_tweets_clean %&gt;% count(word, sort = TRUE) %&gt;% top_n(15) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x = word, y = n)) + geom_col(fill=&#39;steelblue4&#39;) + xlab(NULL) + coord_flip() + labs(x = &quot;Anzahl&quot;, y = &quot;Wörter&quot;, title = &quot;Count of unique words No. 2&quot;, subtitle=&quot;Tweets using #Erziehung&quot;, caption = paste(&quot;Source: Data collected from TwitterAPI via rtweet-package &quot;,Sys.Date())) Wir können erkennen, dass die Daten mit der Entfernung der Stopwörter aussagekräftiger werden in Bezug auf meist verwendeten Wörter. Es ist auch möglich eigene Stopwörter zu entfernen, die in der Häufigkeitstabelle auftachen; wie beispielsweise RT, rt für ReTweet und das Wort Erziehung beziehungsweise erziehung. 8.6 Eigene Stopwörter erstellen Code ownStopwords=c(&quot;RT&quot;,&quot;Erziehung&quot;, &quot;erziehung&quot;,&quot;rt&quot;) ownStopwords&lt;-data.frame(ownStopwords) ownStopwords&lt;-rename (ownStopwords, word=&quot;ownStopwords&quot;) erziehung_tweets_clean &lt;- erziehung_tweets_clean %&gt;% anti_join(ownStopwords, by=&quot;word&quot;) Nun lassen wir uns die Häufigkeitstabelle erneut ausgeben. In diesem Fall wird auf den obigen Code verwiesen. Der Output sollte wie folgt aussehen (die einzelnen Wörter können sich bei dir unterscheiden): 8.7 Data Visualization: Word-Cloud Eine andere Alternative zur Darstellung von Häufigkeitsanalysen ist die Visualisierung der Daten in einer Word-Cloud. Dazu existiert eigens ein Paket wordcloud, welche installiert und eingelesen werden muss. Wie neue Pakete hinzugefügt und eingelesen werden, wurden im vorherigen Kapitel 6.2.1 erläutert. Genau wie in der Erstellung der Häufigkeitstabelle ist es erforderlich die Wörter zu zählen. Der Unterschied ist hierbei, dass der Prozess der Auszählung im Vorfeld durchgeführt werden muss. Über den count-Funktion werden die Wörter gezählt und in dem Objekt erziehung_tweets_count abgespeichert. Code erziehung_tweets_count &lt;- erziehung_tweets_clean %&gt;% count(word, sort = TRUE) Welche Daten das Objekt enthält, erhalten wir über die head-Funktion: Ein tibble-Objekt mit zwei Spalten Wörter und deren Häufigkeit. Code head(erziehung_tweets_count) ## # A tibble: 6 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 findom 41 ## 2 lady__jessica 38 ## 3 00 28 ## 4 oktober 28 ## 5 termin 28 ## 6 dienen 27 Nun zur Visualisierung der Wordcloud. Es wird ein Plot über die Funktion wordclouderzeugt mit den Daten, die wir im vorherigen Schritt vorbereitet haben. Das Argument words wird der Spalte word aus dem Objekt erziehung_tweets_count und die freq wird aus der Spalte der Häufigkeit des Objektes gespeist. Code wordcloud(words = erziehung_tweets_count$word, freq = erziehung_tweets_count$n, max.words = 50, scale = c(2,.5), colors=brewer.pal(6, &quot;Dark2&quot;)) 8.7.1 Aufgabe Die Visualisierungen sind nun auf der Datenbasis des Hashtags #Erziehung realisiert worden. Führe den aufgeführten Prozess mit einem eigenen Hashtag oder mit dem Hashtag #Lyrik durch. 8.8 Quelle Zur Dokumentation des Pakets: stopwords Zur Dokumentation des Pakets: wordcloud 8.9 Feedback "],["dritte-lerneinheit---sentimentanalyse-teil-1.html", "Chapter 9 Dritte Lerneinheit - Sentimentanalyse: Teil 1 9.1 Aufgabe 9.2 Knowledge-Check 9.3 Feedback", " Chapter 9 Dritte Lerneinheit - Sentimentanalyse: Teil 1 Zur Anwendung der Sentimentanalyse ist die inhaltliche Auseinandersetzung mit dieser Art von Analyse essentiell. In diesem Sinne widmet sich das folgende Kapitel mit den inhaltlichen Elementen der Sentimentanalyse. 9.1 Aufgabe Arbeiten Sie den Eintrag zur Sentimentanalyse auf der Website fortext.net durch. Führe schriftlich aus, welche Ziele mit einer Sentimentanalyse verfolgt werden und was den Gegenstand einer Sentimentanalyse ausmacht. 9.2 Knowledge-Check Was bedeutet Sentimentanaylse? Wieso eignen sich insbesondere Literarische Texte für solche Analysen? Nach welchen Methoden wird die Sentimentanalyse durchgeführt? Was sind die Pros und Contras für eine lexikonbasierte Sentimentanalyse? Vor welcher technischen Herausforderung steht die Sentimentanalyse aktuell? 9.3 Feedback "],["dritte-lerneinheit---sentimentanalyse-teil-2.html", "Chapter 10 Dritte Lerneinheit - Sentimentanalyse: Teil 2 10.1 Aufgabe 10.2 Vorbereitung 10.3 Groß- und Kleinschreibung 10.4 Ergebnis des Data Pre-Processing 10.5 Lexikon/Wörterbuch 10.6 Ergänzung der positiven und negativen Sentiments 10.7 Sentimentanalyse 10.8 Visualisierung Sentimentanalyse 10.9 Sentiment-Score 10.10 Word-Cloud 10.11 Feedback", " Chapter 10 Dritte Lerneinheit - Sentimentanalyse: Teil 2 Nach der inhaltlichen Auseinandersetzung mit dem Thema befassen wir uns mit der Realisierung einer Sentimentanalyse anhand unserer Twitter-Daten. 10.1 Aufgabe Versuche die Schritte der beispielhaften Anwendung einer Sentimentanalyse im folgenden Beitrag nachzuvollziehen: towardsdatascience.com 10.2 Vorbereitung Zunächst lesen wir alle Pakete ein, die uns bereits bekannt sind und die wir für die Analyse brauchen. Dies sind rtweet,dplyr,tidytext,ggplot2,wordcloud,gridExtra und stopwords. Die Pakete sind insofern wichtig, da wir ebenfalls die Schritte des Data Pre-Processing wie im vorherigen Kapitel durchführen müssen. Ferner greifen wir erneut auf die Twitter-API, sodass eine Authentizifierung über das entsprechende Paket notwendig ist. Die Analyse wird beispielhaft mit dem Hashtag #Lyrik durchgeführt. In diesem Zusammenhang wird vorausgesetzt, dass: die Suche nach dem #Lyrik bereits vollzogen ist, die Daten bereinigt wurden und die Stopwörter entfernt wurden 10.3 Groß- und Kleinschreibung Für die Analyse werden die Tweets - wie bekannt - in einzelne Wörter zerlegt. Jedoch kommt für diese Analyse ein zusätzlicher Schritt hinzu. Die einzelnen Wörter sollen nicht ins kleingeschriebene konvertiert werden. Das Vorgehen ist insofern notwendig, da das verwendete Lexikon für die Sentimentanalyse kleingeschriebene und großgeschriebenen Wörter kennt. Der Code dafür lautet wie folgt: Code lyrik_tweets_clean &lt;- lyrik_tweets %&gt;% select(stripped_text) %&gt;% unnest_tokens(word, stripped_text, to_lower=FALSE) Die default-Einstellung in dem Vorgehen ist die Wörter ins kleingeschriebene zu konvertieren. 10.4 Ergebnis des Data Pre-Processing Das Ergebnis sollte - in ähnlicher Weise - in einer Häufigkeitstabelle folgendermaßen aussehen: 10.5 Lexikon/Wörterbuch Das Lexikon, welches für die Sentimentanalyse verwendet wird, stammt von der Universität Leipzig: Leipzig Corpora Collection aus dem Projekt Deutscher Wortschatz. Ferner bedienen wir uns für die Importierung des Wörterbuches an einen bereits fertigen Code von dem Github User Polmine, der das Paket data.tablebenötigt. Das Wörterbuch wird anschließend in dem Objekt all_sentiments abgespeichert. Code # Import SentiWS dictionary for sentiment analysis into R as data.table # Source: https://gist.github.com/PolMine/70eeb095328070c18bd00ee087272adf library(data.table) # Definition einer Funktion namens &quot;get_sentiws()&quot; get_sentiws &lt;- function(){ sentiws_tmp_dir &lt;- file.path(tempdir(), &quot;sentiws&quot;) if (!file.exists(sentiws_tmp_dir)) dir.create(sentiws_tmp_dir) sentiws_zipfile &lt;- file.path(sentiws_tmp_dir, &quot;SentiWS_v2.0c.zip&quot;) sentiws_url &lt;- &quot;http://pcai056.informatik.uni-leipzig.de/downloads/etc/SentiWS/SentiWS_v2.0.zip&quot; download.file(url = sentiws_url, destfile = sentiws_zipfile) unzip(zipfile = sentiws_zipfile, exdir = sentiws_tmp_dir) .unfold &lt;- function(.SD){ pos &lt;- gsub(&quot;^([A-Z]+)\\\\s+.*$&quot;, &quot;\\\\1&quot;, .SD[[&quot;data&quot;]][1]) weight &lt;- as.numeric(gsub(&quot;^[A-Z]+\\\\s+(-?\\\\d\\\\.\\\\d+).*$&quot;, &quot;\\\\1&quot;, .SD[[&quot;data&quot;]][1])) words &lt;- gsub(&quot;^[A-Z]+\\\\s+-?\\\\d\\\\.\\\\d+\\\\s*(.*?)\\\\s*$&quot;, &quot;\\\\1&quot;, .SD[[&quot;data&quot;]][1]) words &lt;- if (!grepl(&quot;^\\\\s*$&quot;, words)) strsplit(x = words, split = &quot;,&quot;)[[1]] else NULL list( word = c(.SD[[&quot;word&quot;]][1], words), base = c(TRUE, rep(FALSE, times = length(words))), lemma = .SD[[&quot;word&quot;]][1], pos = pos, weight = weight ) } dts &lt;- lapply( c(positive = &quot;SentiWS_v2.0_Positive.txt&quot;, negative = &quot;SentiWS_v2.0_Negative.txt&quot;), function(filename){ dt &lt;- fread(file.path(sentiws_tmp_dir, filename), sep = &quot;|&quot;) colnames(dt) &lt;- c(&quot;word&quot;, &quot;data&quot;) dt[, &quot;id&quot; := 1L:nrow(dt)] dt[, .unfold(.SD), by = c(&quot;id&quot;)] } ) rbindlist(dts) } #Abspeicherung des Wörterbuches all_sentiments &lt;- get_sentiws() 10.6 Ergänzung der positiven und negativen Sentiments Das bereitgestellte Lexikon der Universität Leipzig besitzt eine ausdifferenzierte numerische Gewichtung über die Bewertung der Wörter als positiv oder als negativ. Für unsere Zwecke vereinfachen wir die Gewichtung und differenzieren nur zwischen positiven und negativen Sentiments. Das heißt wir transformieren ein metrisches Merkmal in ein kategoriales Merkmal. Code all_sentiments_lexikon_neg &lt;- all_sentiments %&gt;% filter(weight &lt;= 0) %&gt;% mutate(sentiment = &quot;negativ&quot;) all_sentiments_lexikon_pos &lt;- all_sentiments %&gt;% filter(weight &gt; 0) %&gt;% mutate(sentiment = &quot;positiv&quot;) all_sentiments_lexikon &lt;- merge(all_sentiments_lexikon_neg, all_sentiments_lexikon_pos, all=T) Anschließend wollen wir uns das gesamte Lexikon anschauen: Code # zeige gesamtes Lexikon head(all_sentiments_lexikon) ## id word base lemma pos weight sentiment ## 1: 1 Abbruch TRUE Abbruch NN -0.0048 negativ ## 2: 1 Abbruche FALSE Abbruch NN -0.0048 negativ ## 3: 1 Abbruches FALSE Abbruch NN -0.0048 negativ ## 4: 1 Abbruchs FALSE Abbruch NN -0.0048 negativ ## 5: 1 Abbrüche FALSE Abbruch NN -0.0048 negativ ## 6: 1 Abbrüchen FALSE Abbruch NN -0.0048 negativ Zur Überprüfung der Transformation werfen wir einen Blick auf die positiven und negativen Sentiments: Code # zeige positive Lexikon-Einträge all_sentiments_lexikon %&gt;% filter (sentiment==&quot;positiv&quot;) ## id word base lemma pos weight sentiment ## 1: 1 Abschluss FALSE Abschluß NN 0.004 positiv ## 2: 1 Abschlusse FALSE Abschluß NN 0.004 positiv ## 3: 1 Abschlusses FALSE Abschluß NN 0.004 positiv ## 4: 1 Abschluß TRUE Abschluß NN 0.004 positiv ## 5: 1 Abschlüsse FALSE Abschluß NN 0.004 positiv ## --- ## 16561: 1643 üppigste FALSE üppig ADJX 0.201 positiv ## 16562: 1643 üppigstem FALSE üppig ADJX 0.201 positiv ## 16563: 1643 üppigsten FALSE üppig ADJX 0.201 positiv ## 16564: 1643 üppigster FALSE üppig ADJX 0.201 positiv ## 16565: 1643 üppigstes FALSE üppig ADJX 0.201 positiv Code # zeige negative Lexikon-Einträge all_sentiments_lexikon %&gt;% filter (sentiment==&quot;negativ&quot;) ## id word base lemma pos weight sentiment ## 1: 1 Abbruch TRUE Abbruch NN -0.0048 negativ ## 2: 1 Abbruche FALSE Abbruch NN -0.0048 negativ ## 3: 1 Abbruches FALSE Abbruch NN -0.0048 negativ ## 4: 1 Abbruchs FALSE Abbruch NN -0.0048 negativ ## 5: 1 Abbrüche FALSE Abbruch NN -0.0048 negativ ## --- ## 18026: 1826 überwältigt FALSE überwältigen VVINF -0.0048 negativ ## 18027: 1826 überwältigte FALSE überwältigen VVINF -0.0048 negativ ## 18028: 1826 überwältigten FALSE überwältigen VVINF -0.0048 negativ ## 18029: 1826 überwältigtest FALSE überwältigen VVINF -0.0048 negativ ## 18030: 1826 überwältigtet FALSE überwältigen VVINF -0.0048 negativ 10.7 Sentimentanalyse Nun können wir mit der eigentlichen Sentimentanalyse fortfahren. Dafür gleichen wir die lyrik-Tweets mit dem Lexikon ab und zählen die Übereinstimmungen. Anschließend geben wir das Ergebnis in dem neuen Objekt sentiment_analysis_lyrik mit dem head()-Befehl aus. Code sentiment_analyse_lyrik&lt;- lyrik_tweets_clean %&gt;% inner_join(all_sentiments_lexikon) %&gt;% count(word, sentiment, sort = T) %&gt;% ungroup() head(sentiment_analyse_lyrik) ## # A tibble: 6 × 3 ## word sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 lernen positiv 108 ## 2 ruhig positiv 19 ## 3 mutiger positiv 16 ## 4 wunderbaren positiv 8 ## 5 Liebe positiv 6 ## 6 hebt positiv 4 10.8 Visualisierung Sentimentanalyse Mithilfe einer Häufigkeitstabelle erhalten wir uns nun einen tieferen Einblick in die Daten. An dieser Stelle soll erneut darauf hingewiesen, dass das Merkmal sentiment als eine kategoriale Variable zu behandeln ist Code sentiment_analyse_lyrik %&gt;% group_by(sentiment) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(word = reorder(word,n)) %&gt;% ggplot(aes(word,n, fill=sentiment)) + geom_col(show.legend = F) + #facet_wrap: Unterscheidung zwischen den beiden Ausprägungen: positiv und negativ. facet_wrap(~sentiment, scales=&quot;free_y&quot;) + labs(title=&quot;Tweets #Lyrik&quot;, y= &quot;Anteil am Sentiment&quot;, x= NULL)+ coord_flip() + theme_bw() 10.9 Sentiment-Score Ferner ermöglicht die Gewichtung des Lexikon die Berechnung eines Sentiment-Scores für den Tweets-Datensatz. Der Sentiment-Score bewertet die Stimmung eines Transkripts beziehungsweise der Tweets in Bezug auf einem Spektrum von positiv (Wert 1) bis negativ (-1) bewerten. Für die Berechnung des Sentiment-Scores werden die Ergebnisse in dem Objekt sentiment_score_lyrik abgespeichert. Code sentiment_score_lyrik&lt;- lyrik_tweets_clean %&gt;% inner_join(all_sentiments_lexikon) %&gt;% count(word, sentiment, weight, sort = T) %&gt;% ungroup() #Berechnung des Scores: Gewichtung * Häufigkeit sentiment_score_lyrik &lt;- sentiment_score_lyrik %&gt;% mutate(score = weight*n) Mithilfe des Histogramms betrachten wir in welche Richtung die Tweets gelagert sind. Dafür lassen wir uns zwei unterschiedliche Histogramme anzeigen. Für die Einteilung des Histogramms existieren bestimmte Faustregeln, die an dieser Stelle nicht weiter erläutert werden. Die Berechnungsgrundlage ist die Wurzel aus der Anzahl an allen Wörtern. Das erste Histogramm teilen wir ein in 9 Klassen ein: Code ggplot(sentiment_score_lyrik, aes(x=weight)) + geom_histogram(bins = 9, alpha = 0.6) + theme_bw() + labs(title=&quot;Histogram Twitter - Sentiment Score&quot;, y=&quot;Anzahl&quot;, x=&quot;Sentiment Score&quot;) Das zweite Histogramm wird in 18 Klassen eingeteilt: Code ggplot(sentiment_score_lyrik, aes(x=weight)) + geom_histogram(bins = 18, alpha = 0.6) + theme_bw() + labs(title=&quot;Histogram Twitter - Sentiment Score&quot;, y=&quot;Anzahl&quot;, x=&quot;Sentiment Score&quot;) In beiden Graphen ist zu erkennen, dass die Mehrheit der Wörter in den Tweets eher neutral sind. Ergänzend lässt sich eine stärkere Gewichtung eines positiven Trends erkennen als eines negativen Trends. Letzteres ist insbesondere im zweiten Histogramm wahrzunehmen. 10.10 Word-Cloud Selbstverständlich ist es möglich die Häufigkeitstabelle in einer Word-Cloud darzustellen. Erstelle jeweils eine Word-Cloud für die beiden Sentiments (positiv und negativ) 10.11 Feedback "],["abschluss-aufgabe.html", "Chapter 11 Abschluss Aufgabe 11.1 Inhalt 11.2 Einführung in RMarkdown 11.3 Feedback", " Chapter 11 Abschluss Aufgabe Zur Dokumentation der Ergebnisse in RStudio eigent sich das Format RMarkdown. RMarkdown basiert auf einer Markdown-Syntax mit dem Ziel eine lesbare Ausgansform vor der Konvertierung in anderes (bekannteres) Format wie einer PDF-Datei oder einer Word-Datei darzustellen. 11.1 Inhalt Analyse eines Hashtags und die Dokumentierung der Ergebnisse oder deines Analyse-Prozesses in einem Rmarkdown-Dokument In dieser Aufgabe werden viele Elemente vorausgesetzt, die in den vorherigen Kapitel dargelegt werden. Dies ist Absicht. Besonders effektiv kann gelerntes durch Anwendung gefestigt werden. An dieser Stelle möchten wir dich ermutigen bei Fragen oder Unsicherheit die entsprechenden Kapiteln anzusteuern, das Feedback-Feld zu nutzen oder deine Kommilitonen_innnen um Rat zu fragen. 11.2 Einführung in RMarkdown Zur Erstellung eines RMarkdown-Dokuments schau dir folgende Ressourcen an: Video-Tutorial von Sebastian Sauer zur Einführung in RMarkdown Schritt für Schritt Anleitung Des Weiteren können wir auf Anfrage ein exemplarisches Dokument bereitstellen. 11.3 Feedback "]]
